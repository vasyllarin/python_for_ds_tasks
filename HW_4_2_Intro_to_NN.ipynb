{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "print(\"Inputs Tensor:\\n\", inputs_tensor)\n",
        "print(\"Targets Tensor:\\n\", targets_tensor)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0c1024-de2a-45a2-9bd7-2fc175728f41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Tensor:\n",
            " tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "Targets Tensor:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314d5de4-e88d-43f2-955c-d735a4f27e59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78fcac434bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 3\n",
        "w = torch.randn(num_features, 1)\n",
        "b = torch.randn(1)\n",
        "print(\"Weights w:\\n\", w)\n",
        "print(\"Bias b:\\n\", b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203b1ea1-7ec3-499e-e27b-4ccb1dc2f4e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights w:\n",
            " tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]])\n",
            "Bias b:\n",
            " tensor([0.6213])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "  z = torch.matmul(x, w) + b\n",
        "  y_hat = 1 / (1 + torch.exp(-z))\n",
        "  return y_hat\n",
        "\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54190df-8fe7-431d-af78-96e5a72aff14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Всі передбачення = 1. Так може бути тому, що модель ще не навчена"
      ],
      "metadata": {
        "id": "ef2LBKMh2Y5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "  eps = 1e-5\n",
        "  predicted_probs = torch.clamp(predicted_probs, eps, 1 - eps)\n",
        "  bce_loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "  return torch.mean(bce_loss)\n",
        "\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "print(\"Binary Cross-Entropy Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987287c3-37d0-4401-cdb4-0252496f3cd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Loss: 4.604633331298828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(3, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "predictions = model(inputs_tensor, w, b)\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "loss.backward()\n",
        "print(\"Gradients for w:\\n\", w.grad)\n",
        "print(\"Gradients for b:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0728729f-f356-4f35-a65b-fabe8b1ef36b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for w:\n",
            " tensor([[nan],\n",
            "        [nan],\n",
            "        [nan]])\n",
            "Gradients for b:\n",
            " tensor([nan])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000\n",
        "\n",
        "predictions_new = model(inputs_tensor, w.T, b)\n",
        "loss_new = binary_cross_entropy(predictions_new, targets_tensor)\n",
        "loss_new.backward()\n",
        "print(\"Predictions:\\n\", predictions_new)\n",
        "print(\"\\nLoss:\\n\", loss_new.item())\n",
        "print(\"\\nGradients для w:\\n\", w.grad)\n",
        "print(\"\\nGradients для b:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8da436-6823-4151-c470-b1218676b2b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            " tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "\n",
            "Loss:\n",
            " 0.6829456686973572\n",
            "\n",
            "Gradients для w:\n",
            " tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "\n",
            "Gradients для b:\n",
            " tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# генерація прогнозів\n",
        "preds = model(inputs_tensor, w.T, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0d2c78-da9d-4585-e750-0df6dadf6e5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# обчислення втрат\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()\n",
        "targets_torch = torch.from_numpy(targets).to(preds.dtype)\n",
        "loss = mse(preds, targets_torch)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUGs0Civw9iS",
        "outputId": "3410b414-c2c7-4928-c5c2-414edd82298f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2449, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислення градієнтів\n",
        "loss.backward()\n",
        "print(\"\\nGradients для w:\\n\", w.grad)"
      ],
      "metadata": {
        "id": "9FznTWPxw9ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f07fc9-e290-49d7-93c4-0760ac14b43b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradients для w:\n",
            " tensor([[-10.8641, -37.9264, -20.1134]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY9XqRo4K0Pg",
        "outputId": "8f8c6335-2252-4b1c-a2dc-362ecb4b216e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0008, 0.0006, 0.0003]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs_tensor, w.T, b)\n",
        "targets_torch = torch.from_numpy(targets).to(preds.dtype)\n",
        "loss = mse(preds, targets_torch)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPsIfJWJK8cx",
        "outputId": "ef20f9bf-0e7b-4df7-e7d3-176eaeb11930"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2402, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Тренування протягом 1000 епох\n",
        "for i in range(1000):\n",
        "    preds = model(inputs_tensor, w.T, b)\n",
        "    targets_torch = torch.from_numpy(targets).to(preds.dtype)\n",
        "    loss = mse(preds, targets_torch)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "preds = model(inputs_tensor, w.T, b)\n",
        "targets_torch = torch.from_numpy(targets).to(preds.dtype)\n",
        "loss = mse(preds, targets_torch)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j75I8WCZK8S3",
        "outputId": "98db3032-751b-4ea2-ded2-69061ac69eac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1269, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ean7OIuIMFtu",
        "outputId": "17c6755f-00b6-4b8b-b7cc-83775cb74f41"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5716],\n",
              "        [0.6329],\n",
              "        [0.8222],\n",
              "        [0.2982],\n",
              "        [0.7709]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets_torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSxmsDeLMEul",
        "outputId": "978adb24-36a6-497b-fd9b-ccfb545fe3ba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 передбачення із 5 є точними (точність 80%)"
      ],
      "metadata": {
        "id": "eLGGg1a5MQs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "train_ds = TensorDataset(inputs_tensor, targets_tensor)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ee78ca-675f-4a03-b5ad-55241fdac357"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b192d532-b926-4e70-f03a-704c4c9b2eb1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[102.,  43.,  37.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "model = LogReg()"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = F.binary_cross_entropy\n",
        "inputs_tensor = torch.from_numpy(inputs).float()\n",
        "inputs_tensor = (inputs_tensor - inputs_tensor.mean(0)) / inputs_tensor.std(0) # нормалізація\n",
        "targets_tensor = torch.from_numpy(targets).float()\n",
        "preds = model(inputs_tensor)\n",
        "loss_value = loss_fn(preds, targets_tensor)\n",
        "print(loss_value.item())"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc69116-1715-4fda-ec50-22ab026bcee2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7763237357139587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Моделі не вдалося навчитися, оскільки значення loss стало вище"
      ],
      "metadata": {
        "id": "EgrP5dZOW0Vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Нормалізація\n",
        "inputs_tensor = torch.from_numpy(inputs).float()\n",
        "inputs_tensor = (inputs_tensor - inputs_tensor.mean(0)) / inputs_tensor.std(0)\n",
        "targets_tensor = torch.from_numpy(targets).float()\n",
        "\n",
        "train_ds = TensorDataset(inputs_tensor, targets_tensor)\n",
        "train_dl = DataLoader(train_ds, batch_size=5, shuffle=True)\n",
        "\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_dl:\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses\n",
        "\n",
        "losses = fit_return_loss(1000, model, loss_fn, opt, train_dl)\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Епоха\")\n",
        "plt.ylabel(\"Loss (Binary Cross Entropy)\")\n",
        "plt.title(\"Зміна loss під час навчання\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cEHQH9qE626k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e7c3cc0-7ad2-4744-c77d-74dc5f4bd423"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 0.6614\n",
            "Epoch [20/1000], Loss: 0.5628\n",
            "Epoch [30/1000], Loss: 0.4864\n",
            "Epoch [40/1000], Loss: 0.4263\n",
            "Epoch [50/1000], Loss: 0.3787\n",
            "Epoch [60/1000], Loss: 0.3407\n",
            "Epoch [70/1000], Loss: 0.3089\n",
            "Epoch [80/1000], Loss: 0.2828\n",
            "Epoch [90/1000], Loss: 0.2609\n",
            "Epoch [100/1000], Loss: 0.2420\n",
            "Epoch [110/1000], Loss: 0.2257\n",
            "Epoch [120/1000], Loss: 0.2115\n",
            "Epoch [130/1000], Loss: 0.1990\n",
            "Epoch [140/1000], Loss: 0.1879\n",
            "Epoch [150/1000], Loss: 0.1779\n",
            "Epoch [160/1000], Loss: 0.1690\n",
            "Epoch [170/1000], Loss: 0.1609\n",
            "Epoch [180/1000], Loss: 0.1536\n",
            "Epoch [190/1000], Loss: 0.1469\n",
            "Epoch [200/1000], Loss: 0.1407\n",
            "Epoch [210/1000], Loss: 0.1350\n",
            "Epoch [220/1000], Loss: 0.1298\n",
            "Epoch [230/1000], Loss: 0.1250\n",
            "Epoch [240/1000], Loss: 0.1205\n",
            "Epoch [250/1000], Loss: 0.1163\n",
            "Epoch [260/1000], Loss: 0.1124\n",
            "Epoch [270/1000], Loss: 0.1087\n",
            "Epoch [280/1000], Loss: 0.1053\n",
            "Epoch [290/1000], Loss: 0.1021\n",
            "Epoch [300/1000], Loss: 0.0990\n",
            "Epoch [310/1000], Loss: 0.0962\n",
            "Epoch [320/1000], Loss: 0.0935\n",
            "Epoch [330/1000], Loss: 0.0909\n",
            "Epoch [340/1000], Loss: 0.0885\n",
            "Epoch [350/1000], Loss: 0.0862\n",
            "Epoch [360/1000], Loss: 0.0840\n",
            "Epoch [370/1000], Loss: 0.0819\n",
            "Epoch [380/1000], Loss: 0.0799\n",
            "Epoch [390/1000], Loss: 0.0780\n",
            "Epoch [400/1000], Loss: 0.0763\n",
            "Epoch [410/1000], Loss: 0.0745\n",
            "Epoch [420/1000], Loss: 0.0729\n",
            "Epoch [430/1000], Loss: 0.0713\n",
            "Epoch [440/1000], Loss: 0.0698\n",
            "Epoch [450/1000], Loss: 0.0683\n",
            "Epoch [460/1000], Loss: 0.0670\n",
            "Epoch [470/1000], Loss: 0.0656\n",
            "Epoch [480/1000], Loss: 0.0643\n",
            "Epoch [490/1000], Loss: 0.0631\n",
            "Epoch [500/1000], Loss: 0.0619\n",
            "Epoch [510/1000], Loss: 0.0608\n",
            "Epoch [520/1000], Loss: 0.0597\n",
            "Epoch [530/1000], Loss: 0.0586\n",
            "Epoch [540/1000], Loss: 0.0576\n",
            "Epoch [550/1000], Loss: 0.0566\n",
            "Epoch [560/1000], Loss: 0.0556\n",
            "Epoch [570/1000], Loss: 0.0547\n",
            "Epoch [580/1000], Loss: 0.0538\n",
            "Epoch [590/1000], Loss: 0.0529\n",
            "Epoch [600/1000], Loss: 0.0521\n",
            "Epoch [610/1000], Loss: 0.0512\n",
            "Epoch [620/1000], Loss: 0.0504\n",
            "Epoch [630/1000], Loss: 0.0497\n",
            "Epoch [640/1000], Loss: 0.0489\n",
            "Epoch [650/1000], Loss: 0.0482\n",
            "Epoch [660/1000], Loss: 0.0475\n",
            "Epoch [670/1000], Loss: 0.0468\n",
            "Epoch [680/1000], Loss: 0.0462\n",
            "Epoch [690/1000], Loss: 0.0455\n",
            "Epoch [700/1000], Loss: 0.0449\n",
            "Epoch [710/1000], Loss: 0.0443\n",
            "Epoch [720/1000], Loss: 0.0437\n",
            "Epoch [730/1000], Loss: 0.0431\n",
            "Epoch [740/1000], Loss: 0.0425\n",
            "Epoch [750/1000], Loss: 0.0420\n",
            "Epoch [760/1000], Loss: 0.0414\n",
            "Epoch [770/1000], Loss: 0.0409\n",
            "Epoch [780/1000], Loss: 0.0404\n",
            "Epoch [790/1000], Loss: 0.0399\n",
            "Epoch [800/1000], Loss: 0.0394\n",
            "Epoch [810/1000], Loss: 0.0389\n",
            "Epoch [820/1000], Loss: 0.0385\n",
            "Epoch [830/1000], Loss: 0.0380\n",
            "Epoch [840/1000], Loss: 0.0376\n",
            "Epoch [850/1000], Loss: 0.0372\n",
            "Epoch [860/1000], Loss: 0.0367\n",
            "Epoch [870/1000], Loss: 0.0363\n",
            "Epoch [880/1000], Loss: 0.0359\n",
            "Epoch [890/1000], Loss: 0.0355\n",
            "Epoch [900/1000], Loss: 0.0351\n",
            "Epoch [910/1000], Loss: 0.0348\n",
            "Epoch [920/1000], Loss: 0.0344\n",
            "Epoch [930/1000], Loss: 0.0340\n",
            "Epoch [940/1000], Loss: 0.0337\n",
            "Epoch [950/1000], Loss: 0.0333\n",
            "Epoch [960/1000], Loss: 0.0330\n",
            "Epoch [970/1000], Loss: 0.0326\n",
            "Epoch [980/1000], Loss: 0.0323\n",
            "Epoch [990/1000], Loss: 0.0320\n",
            "Epoch [1000/1000], Loss: 0.0317\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_preds = model(inputs_tensor)\n",
        "print(\"Фінальні передбачення:\")\n",
        "print(final_preds.detach().numpy())\n",
        "\n",
        "print(\"Таргети:\")\n",
        "print(targets_tensor.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-DhSk9TZnbo",
        "outputId": "22a2777a-015d-4976-a2df-c3e061f40f47"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фінальні передбачення:\n",
            "[[0.05338515]\n",
            " [0.9448346 ]\n",
            " [0.98651636]\n",
            " [0.01051226]\n",
            " [0.977693  ]\n",
            " [0.05338515]\n",
            " [0.9448346 ]\n",
            " [0.98651636]\n",
            " [0.01051226]\n",
            " [0.977693  ]\n",
            " [0.05338515]\n",
            " [0.9448346 ]\n",
            " [0.98651636]\n",
            " [0.01051226]\n",
            " [0.977693  ]]\n",
            "Таргети:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n"
          ]
        }
      ]
    }
  ]
}